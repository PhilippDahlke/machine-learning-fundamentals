{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTW-Berlin - Informatik und Wirtschaft - Aktuelle Trends - Machine Learning: Logistic Regression Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Introduction](#Introduction)\n",
    "  * [Required Knowledge](#Required-Knowledge)\n",
    "  * [Required Python Modules](#Required-Python-Modules)\n",
    "\n",
    "\n",
    "* [Logistic Regression](#Linear-Regression)\n",
    "  * [Exercise: Sigmoid](#Exercise:-Sigmoid)\n",
    "  * [Exercise: Hypothesis](#Exercise:-Hypothesis)\n",
    "  + [Exercise: Classifier](#Exercise:-Classifier)\n",
    "  * [Exercise: Cost](#Exercise:-Cost)\n",
    "  * [Exercise: Gradient](#Exercise:-Gradient)\n",
    "  * [Exercise: Stochastic Gradient Descent](#Exercise:-Stochastic-Gradient-Descent)\n",
    "  * [Exercise: Plot Cost per Epoch](#Exercise:-Plot-Cost-per-Epoch)\n",
    "  * [Plot Boundary](#Plot-Boundary)\n",
    "\n",
    "\n",
    "* [Summary and Outlook](#Summary-and-Outlook)\n",
    "* [Licenses](#Licenses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Goal of this exercise is to implement Logistic Regression in Python. You can use the Python standard library and math functions from numpy. This notebook guides you through the implementation process.\n",
    "\n",
    "This notebooks implements tests using `assert` or `np.testing.assert_almost_equal`. If you run the corresponding notebook cell and no output appears, the test has passed. Otherwise an exception is raised.\n",
    "\n",
    "**General Hint:**\n",
    "\n",
    "If you have problems with the implementation (e.g. you don't know how to call a certain function or you don't know how to loop through the dataset), make use of the interactive nature of the notebook. You can at all times add new cells to the notebook to inspect defined variables or to try small code snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Knowledge\n",
    "\n",
    "This exercise is part of the course \"Aktuelle Trends der Informations- und Kommunikationstechnik\". The fundamentals of Logistic Regression are taught in class.\n",
    "\n",
    "* The PDF slides used in class are [available](../../../../../slides/courses/htw-berlin/informatik-und-wirtschaft/aktuelle-trends/logistic-regression.pdf) in the educational-materials repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from deep_teaching_commons.data.fundamentals.iris import Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_dir = os.path.expanduser('~/deep.TEACHING/data')\n",
    "dm = Iris(base_data_dir=base_data_dir)  # data manager\n",
    "iris = dm.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = iris.query('species == \"Iris-versicolor\" | species == \"Iris-virginica\"')\n",
    "df_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reduced[['petal_width', 'petal_length']].values\n",
    "Y = df_reduced['species'].replace({'Iris-versicolor': 0, 'Iris-virginica': 1}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = sns.color_palette()\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=['x1', 'x2'])\n",
    "df_scaled['y'] = Y\n",
    "sns.scatterplot(data=df_scaled, x='x1', y='x2', hue='y', palette=cp[1:3]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Sigmoid\n",
    "\n",
    "The sigmoid function is defined as follows:\n",
    "\n",
    "$$\n",
    "sigmoid(t) = \\frac{1}{1 + e^{-t}}\n",
    "$$\n",
    "\n",
    "Implement this function below. You should use `np.exp` to calculate $e^{-t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    raise NotImplementedError('implement this function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tests\n",
    "np.testing.assert_almost_equal(sigmoid(0), 0.5)\n",
    "np.testing.assert_almost_equal(sigmoid(1), 0.7310585786300049)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = np.linspace(-7, 7, 100)\n",
    "plt.plot(spacing, [sigmoid(t) for t in spacing]);\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('sigmoid(t)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Hypothesis\n",
    "\n",
    "The logistic hypothesis is defined as follows:\n",
    "\n",
    "$$\n",
    "h(x_1, x_2) = sigmoid(x_1 w_1 + x_2 w_2 + b)\n",
    "$$\n",
    "\n",
    "The hypothesis is a function of $x_1$ and $x_2$, where the parameters $w_1$, $w_2$ and $b$ are treated as constants.\n",
    "\n",
    "Implement this function in Python as a closure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_logistic_hypothesis(w1, w2, b):\n",
    "    # this is a closure\n",
    "    def logistic_hypothesis(x1, x2):\n",
    "        raise NotImplementedError('implement this function')\n",
    "    \n",
    "    return logistic_hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2, b = 3.3, 3.7, 0.3\n",
    "h = make_logistic_hypothesis(w1, w2, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypothesis $h$ is used to predict values for $y$ given $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = -1, 1\n",
    "h(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a test\n",
    "np.testing.assert_almost_equal(h(-1, 1), 0.6681877721681662)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Classifier\n",
    "\n",
    "The logistic hypothesis calculates probability values in range $]0, 1[$. In order to decide wether a dataset is classfied as $1$ or $0$, we need to define a threshold as decision boundary. The function `classify` uses this threshold and is defined as follows.\n",
    "\n",
    "$$\n",
    "classify(x{_1}, x{_2}) \\cases{1, & if $h(x_1, x_2) > threshold$ \\\\ 0, & otherwise}\n",
    "$$\n",
    "\n",
    "Implement `classify` in Python as a closure. You can use the already implemented function `logistic_hypothesis` inside of `classify` to create a new hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classify(w1, w2, b, threshold):\n",
    "    def classify(x1, x2):\n",
    "        raise NotImplementedError('implement this function')\n",
    "    \n",
    "    return classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2, b = 3.3, 3.7, 0.3\n",
    "threshold = 0.5\n",
    "classify = make_classify(w1, w2, b, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = -1, -1\n",
    "classify(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = 1, 1\n",
    "classify(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tests\n",
    "assert classify(-1, -1) == 0\n",
    "assert classify(1, 1) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Cost\n",
    "\n",
    "The cost function is defined as follows:\n",
    "\n",
    "$$\n",
    "J(w1, w2, b) = \\frac{1}{m} \\sum_{i=1}^{m} y^{i} \\cdot -ln(h(x_{1}^{i}, x_{2}^{i})) +  (1 - y^{i}) \\cdot -ln(1 - h(x_{1}^{i}, x_{2}^{i}))\n",
    "$$\n",
    "\n",
    "Implement this function using a closure. The binary crossentropy cost function $J$ loops through the complete dataset of X and Y, to sum over the calculated cost values. As a last step the summation is divided by $m$, where $m$ is the number of samples in $X$.\n",
    "\n",
    "$J$ is a function of $w_1$, $w_2$ and $b$, because we want to find the best parameters $w_1$, $w_2$ and $b$ providing the lowest possible cost. Therefore the data $X$ and $Y$ is treated as a constant. You can use the already implemented function `logistic_hypothesis` inside of $J$ to create a new hypothesis. Use numpy's build-in function `np.log` to calculate the logarithmus naturalis $ln$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary_crossentropy_cost(X, Y):\n",
    "    def binary_crossentropy_cost(w1, w2, b):\n",
    "        raise NotImplementedError('implement this function')\n",
    "    \n",
    "    return binary_crossentropy_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = make_binary_crossentropy_cost(X_scaled, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2, b = 3.3, 3.7, 0.3\n",
    "J(w1, w2, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a test\n",
    "np.testing.assert_almost_equal(J(3.3, 3.7, 0.3), 0.10736084146625315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Gradient\n",
    "\n",
    "The partial derivatives (gradient) are used by the Stochastic Gradient Descent optimizer and are defined as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial w_1} J(w_{1}, w_{2}, b) &= \\frac{1}{m}\\sum_{i=1}^{m}(h(x_{1}^{i}, x_{2}^{i}) - y^{i}) \\cdot x_{1}^{i}\\\\\n",
    "\\frac{\\partial}{\\partial w_2} J(w_{1}, w_{2}, b) &= \\frac{1}{m}\\sum_{i=1}^{m}(h(x_{1}^{i}, x_{2}^{i}) - y^{i}) \\cdot x_{2}^{i}\\\\\n",
    "\\frac{\\partial}{\\partial b} J(w_{1}, w_{2}, b) &= \\frac{1}{m}\\sum_{i=1}^{m}(h(x_{1}^{i}, x_{2}^{i}) - y^{i})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Implement a function `gradient`, which calculates at a point $w1, w2, b$ the partial derivatives $pd\\_w1$, $pd\\_w2$ and $pd\\_b$. Return all three values from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradient(X, Y):\n",
    "    def gradient(w1, w2, b):\n",
    "        raise NotImplementedError('implement this function')\n",
    "        \n",
    "        pd_w1 = None\n",
    "        pd_w2 = None\n",
    "        pd_b = None\n",
    "        \n",
    "        return pd_w1, pd_w2, pd_b\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = make_gradient(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2, b = 3.3, 3.7, 0.3\n",
    "gradient(w1, w2, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a test\n",
    "np.testing.assert_almost_equal(gradient(3.3, 3.7, 0.3), (0.6629999910605702, 2.129999972002591, 0.49999999158653635))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Stochastic Gradient Descent\n",
    "\n",
    "The following pseude code shows the iterative parameter updates of Stochastic Gradient Descent:\n",
    "\n",
    "---\n",
    "\n",
    "Randomly initialize w and b.\n",
    "\n",
    "For a number of epochs repeat:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "pd\\_w1 &:= \\frac{\\partial}{\\partial w_1} J(w_1, w_2, b)\\\\\n",
    "pd\\_w2 &:= \\frac{\\partial}{\\partial w_2} J(w_1, w_2, b)\\\\\n",
    "pd\\_b &:= \\frac{\\partial}{\\partial b} J(w, b)\\\\\\\\\n",
    "w1 &:= w1 - \\alpha *  pd\\_w1\\\\\n",
    "w2 &:= w2 - \\alpha *  pd\\_w2\\\\\n",
    "b &:= b - \\alpha * pd\\_b\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "The function to be implemented is `stochastic_gradient_descent(X, Y, w1, w2, b, alpha, epochs)`, where `X, Y` is the data, `w1, w2, b` are the randomly initialized parameters, `alpha` is the learning rate and `epochs` is the number of training iterations. You should return the values of $w_1$, $w_2$ and $b$, as well as a list of the cost after each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X, Y, w1, w2, b, alpha, epochs):\n",
    "    raise NotImplementedError('implement this function')\n",
    "    \n",
    "    cost_per_epoch = []\n",
    "    \n",
    "    return w1, w2, b, cost_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "epochs = 1500\n",
    "w1, w2, b = np.random.randn(3)\n",
    "w1, w2, b, cost_per_epoch = sgd(X_scaled, Y, w1, w2, b, alpha, epochs)\n",
    "w1, w2, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cost_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tests\n",
    "test_w1, test_w2, test_b, test_cost_per_epoch = sgd(X_scaled, Y, -1.58979407,  0.26957035, -1.92309864, 0.1, 1500)\n",
    "print(test_w1, test_w2, test_b)\n",
    "np.testing.assert_almost_equal(len(test_cost_per_epoch), 1500)\n",
    "np.testing.assert_almost_equal((test_w1, test_w2, test_b), (3.234979367208197, 3.5709689275028116, 0.2978914362922387))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Plot Cost per Epoch\n",
    "\n",
    "Plot the `cost_per_epoch` result of `sgd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_over_time(cost_per_epoch):\n",
    "    raise NotImplementedError('implement this function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_over_time(cost_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try out different `alpha` values and see how the training performance changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Boundary\n",
    "\n",
    "This is not an exercise. Run the code below to visualize the decision boundary your implementation of Logistic Regression determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decision_boundary(w1, w2, b, threshold):\n",
    "    def decision_boundary(x1):\n",
    "        return (np.log(threshold / (1 - threshold)) - x1*w1 - b) * (1 / w2)\n",
    "    \n",
    "    return decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary(df, decision_boundary):\n",
    "    sns.scatterplot(data=df, x='x1', y='x2', hue='y', palette=sns.color_palette()[1:3])\n",
    "    \n",
    "    spacing = np.linspace(df['x1'].min(), df['x1'].max(), 10)\n",
    "    boundary_values = np.array([decision_boundary(x1) for x1 in spacing])\n",
    "\n",
    "    plt.plot(spacing, boundary_values, label='boundary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary(df_scaled, make_decision_boundary(w1, w2, b, threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Outlook\n",
    "\n",
    "You have learned how to implement Logistic Regression with two inputs and one output variable to solve simple classification problems. The algorithms were implemented in Python, without the help of higher level libraries like Tensorflow or Keras.\n",
    "\n",
    "The next part of the course covers evaluation scores for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licenses\n",
    "\n",
    "### Notebook License (CC-BY-SA 4.0)\n",
    "\n",
    "*The following license applies to the complete notebook, including code cells. It does however not apply to any referenced external media (e.g. images).*\n",
    "\n",
    "HTW-Berlin - Informatik und Wirtschaft - Aktuelle Trends - Machine Learning: Logistic Regression Exercise<br/>\n",
    "by [Christoph Jansen (deep.TEACHING - HTW Berlin)](https://www.htw-berlin.de/hochschule/personen/person/?eid=9225)<br/>\n",
    "is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).<br/>\n",
    "Based on a work at https://gitlab.com/deep.TEACHING.\n",
    "\n",
    "\n",
    "### Code License (MIT)\n",
    "\n",
    "*The following license only applies to code cells of the notebook.*\n",
    "\n",
    "Copyright 2018 Christoph Jansen\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "deep_teaching_kernel",
   "language": "python",
   "name": "deep_teaching_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
