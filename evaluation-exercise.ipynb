{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTW-Berlin - Informatik und Wirtschaft - Aktuelle Trends - Machine Learning: Evaluation Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#Introduction)\n",
    "  * [Required Knowledge](#Required-Knowledge)\n",
    "  * [Required Python Modules](#Required-Python-Modules)\n",
    "* [Evaluation](#Evaluation)\n",
    "  * [Implementation of Logistic Hypothesis](#Implementation-of-Logistic-Hypothesis)\n",
    "  * [Classify Datasets](#Classify-Datasets)\n",
    "  * [Exercise: Accuracy](#Exercise:-Accuracy)\n",
    "  * [Exercise: TP, FP, TN, FN](#Exercise:-TP,-FP,-TN,-FN)\n",
    "  * [Exercise: Precision and Recall](#Exercise:-Precision-and-Recall)\n",
    "  * [Exercise: F-Score](#Exercise:-F-Score)\n",
    "* [Licenses](#Licenses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Goal of this exercise is to learn evaluation scores for a classification task in Python. You can use the Python standard library and math functions from numpy. This notebook guides you through the implementation process.\n",
    "\n",
    "This notebooks implements tests using `assert` or `np.testing.assert_almost_equal`. If you run the corresponding notebook cell and no output appears, the test has passed. Otherwise an exception is raised.\n",
    "\n",
    "**General Hint:**\n",
    "\n",
    "If you have problems with the implementation (e.g. you don't know how to call a certain function or you don't know how to loop through the dataset), make use of the interactive nature of the notebook. You can at all times add new cells to the notebook to inspect defined variables or to try small code snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Knowledge\n",
    "\n",
    "This exercise is part of the course \"Aktuelle Trends der Informations- und Kommunikationstechnik\". The fundamentals of evaluation metrics are taught in class.\n",
    "\n",
    "* The PDF slides used in class are [available](./evaluation.pdf) in the educational-materials repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from deep_teaching_commons.data.fundamentals.iris import Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Data\n",
    "\n",
    "First we load our dataset and do some preprocessing like reducing the number of classes to two, make the class label binary, scale our features and finally divide our dataset into train- and test set. No exercise yet, just execute the cells and try to understand the code. All these preprocessing steps are very common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_dir = os.path.expanduser('~/deep.TEACHING/data')\n",
    "dm = Iris(base_data_dir=base_data_dir)  # data manager\n",
    "iris = dm.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = iris.query('species == \"Iris-versicolor\" | species == \"Iris-virginica\"')\n",
    "df_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reduced[['petal_width', 'petal_length']].values\n",
    "Y = df_reduced['species'].replace({'Iris-versicolor': 0, 'Iris-virginica': 1}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "cp = sns.color_palette()\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=['x1', 'x2'])\n",
    "df_scaled['y'] = Y\n",
    "sns.scatterplot(data=df_scaled, x='x1', y='x2', hue='y', palette=cp[1:3]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data for training and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_scaled, Y, stratify=Y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = dict(zip(*np.unique(Y_train, return_counts=True)))\n",
    "test_classes = dict(zip(*np.unique(Y_test, return_counts=True)))\n",
    "\n",
    "train_classes, test_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Logistic Hypothesis\n",
    "\n",
    "This implementation was part of the last notebook \"Exercise: Logistic Regression\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1 / (1 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_logistic_hypothesis(w1, w2, b):\n",
    "    def logistic_hypothesis(x1, x2):\n",
    "        return sigmoid(x1 * w1 + x2 * w2 + b)\n",
    "    \n",
    "    return logistic_hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decision_boundary(w1, w2, b, threshold):\n",
    "    def decision_boundary(x1):\n",
    "        return (np.log(threshold / (1 - threshold)) - x1*w1 - b) * (1 / w2)\n",
    "    \n",
    "    return decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary(df, decision_boundary):\n",
    "    sns.scatterplot(data=df, x='x1', y='x2', hue='y', palette=sns.color_palette()[1:3])\n",
    "    \n",
    "    spacing = np.linspace(df['x1'].min(), df['x1'].max(), 10)\n",
    "    boundary_values = np.array([decision_boundary(x1) for x1 in spacing])\n",
    "\n",
    "    plt.plot(spacing, boundary_values, label='boundary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classify(w1, w2, b, threshold):\n",
    "    h = make_logistic_hypothesis(w1, w2, b)\n",
    "    \n",
    "    def classify(x1, x2):\n",
    "        return 1 if h(x1, x2) > threshold else 0\n",
    "    \n",
    "    return classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose some mediocre values, to demonstrate metrics\n",
    "w1, w2, b = 3.6962765211562245, 2.548083316850051, 0.01089234547433182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training data\n",
    "df_train = pd.DataFrame(X_train, columns=['x1', 'x2'])\n",
    "df_train['y'] = Y_train\n",
    "plot_boundary(df_train, make_decision_boundary(w1, w2, b, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot test data\n",
    "df_test = pd.DataFrame(X_test, columns=['x1', 'x2'])\n",
    "df_test['y'] = Y_test\n",
    "plot_boundary(df_test, make_decision_boundary(w1, w2, b, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classifier\n",
    "classify = make_classify(w1, w2, b, 0.5)\n",
    "classify(-1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_train = np.array([classify(x1, x2) for x1, x2 in X_train])\n",
    "C_test = np.array([classify(x1, x2) for x1, x2 in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_train.shape, C_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Accuracy\n",
    "\n",
    "Accuracy is defined as\n",
    "\n",
    "$$\n",
    "accuracy = \\frac{T}{T + F}\n",
    "$$\n",
    "\n",
    "where $T$ is the number of true classifications and $F$ is the number of false classifications on a dataset.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Implement the function `accuracy` below. `C` contains the preditions of your classifier and `Y` the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(C, Y):\n",
    "    raise NotImplementedError('implement this function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = accuracy(C_train, Y_train)\n",
    "train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = accuracy(C_test, Y_test)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(accuracy(C_train, Y_train), 0.975)\n",
    "np.testing.assert_almost_equal(accuracy(C_test, Y_test), 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: TP, FP, TN, FN\n",
    "\n",
    "The following table shows a confusion matrix for the results of a binary classifier. \n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            -\n",
    "        </td>\n",
    "        <td>\n",
    "            Pos. labeled\n",
    "        </td>\n",
    "        <td>\n",
    "            Neg. labeled\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Pos. predicted\n",
    "        </td>\n",
    "        <td>\n",
    "            TP\n",
    "        </td>\n",
    "        <td>\n",
    "            FP\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Neg. predicted\n",
    "        </td>\n",
    "        <td>\n",
    "            FN\n",
    "        </td>\n",
    "        <td>\n",
    "            TN\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### True Positive (TP)\n",
    "\n",
    "Number of **true** (T) classifications where the classification result is class **1** (P) bestimmt wurden.\n",
    "\n",
    "#### False Positive (FP)\n",
    "\n",
    "Number of **false** (F) classifications where the classification result is class **1** (P) bestimmt wurden.\n",
    "\n",
    "#### True Negative (TN)\n",
    "\n",
    "Number of **true** (T) classifications where the classification result is class **0** (N) bestimmt wurden.\n",
    "\n",
    "#### False Negative (FN)\n",
    "\n",
    "Number of **false** (F) classifications where the classification result is class **0** (N) bestimmt wurden.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Implement the function `tp_fp_tn_fn` to calculate values for TP, FP, FN, FN on a dataset. Again `C` contains the preditions of your classifier and `Y` the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_fp_tn_fn(C, Y):\n",
    "    raise NotImplementedError('implement this function')\n",
    "    \n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "                \n",
    "    return tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tp, train_fp, train_tn, train_fn = tp_fp_tn_fn(C_train, Y_train)\n",
    "train_tp, train_fp, train_tn, train_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tp, test_fp, test_tn, test_fn = tp_fp_tn_fn(C_test, Y_test)\n",
    "test_tp, test_fp, test_tn, test_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(tp_fp_tn_fn(C_train, Y_train), (39, 1, 39, 1))\n",
    "np.testing.assert_almost_equal(tp_fp_tn_fn(C_test, Y_test), (7, 1, 9, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Precision and Recall\n",
    "\n",
    "Precision and recall are defined as follows:\n",
    "\n",
    "$$\n",
    "precision = \\frac{TP}{TP+FP}\n",
    "$$\n",
    "\n",
    "$$\n",
    "recall = \\frac{TP}{TP+FN}\n",
    "$$\n",
    "\n",
    "Implement a function `precision_recall` to calculate precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall(tp, fp, fn):\n",
    "    raise NotImplementedError('implement this function')\n",
    "    \n",
    "    precision = None\n",
    "    recall = None\n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_precision, train_recall = precision_recall(train_tp, train_fp, train_fn)\n",
    "train_precision, train_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_precision, test_recall = precision_recall(test_tp, test_fp, test_fn)\n",
    "test_precision, test_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(precision_recall(39, 1, 1), (0.975, 0.975))\n",
    "np.testing.assert_almost_equal(precision_recall(7, 1, 3), (0.875, 0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: F-Score\n",
    "\n",
    "The f-score metric is defined as follows:\n",
    "\n",
    "$$\n",
    "F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{precision \\cdot recall}{(\\beta^2 \\cdot precision) + recall}\n",
    "$$\n",
    "\n",
    "Implement a function `f_score` to calculate the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_score(precision, recall, beta):\n",
    "    raise NotImplementedError('implement this function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f1_score = f_score(train_precision, train_recall, 1)\n",
    "train_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f05_score = f_score(train_precision, train_recall, 0.5)\n",
    "train_f05_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_f1_score = f_score(test_precision, test_recall, 1)\n",
    "test_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_f05_score = f_score(test_precision, test_recall, 0.5)\n",
    "test_f05_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(f_score(train_precision, train_recall, 1), 0.975)\n",
    "np.testing.assert_almost_equal(f_score(train_precision, train_recall, 0.5), 0.975)\n",
    "np.testing.assert_almost_equal(f_score(test_precision, test_recall, 1), 0.7777777777777777)\n",
    "np.testing.assert_almost_equal(f_score(test_precision, test_recall, 0.5), 0.8333333333333334)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licenses\n",
    "\n",
    "### Notebook License (CC-BY-SA 4.0)\n",
    "\n",
    "*The following license applies to the complete notebook, including code cells. It does however not apply to any referenced external media (e.g. images).*\n",
    "\n",
    "HTW-Berlin - Informatik und Wirtschaft - Aktuelle Trends - Machine Learning: Evaluation Exercise<br/>\n",
    "by [Christoph Jansen (deep.TEACHING - HTW Berlin)](https://www.htw-berlin.de/hochschule/personen/person/?eid=9225)<br/>\n",
    "is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).<br/>\n",
    "Based on a work at https://gitlab.com/deep.TEACHING.\n",
    "\n",
    "\n",
    "### Code License (MIT)\n",
    "\n",
    "*The following license only applies to code cells of the notebook.*\n",
    "\n",
    "Copyright 2018 Christoph Jansen\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "deep_teaching_kernel",
   "language": "python",
   "name": "deep_teaching_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
