{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Exercise - Univariate Gaussian Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#Introduction)\n",
    "* [Requirements](#Requirements) \n",
    "  * [Python Modules](#Python-Modules)\n",
    "  * [Knowledge](#Knowledge)\n",
    "  * [Modules](#Python-Modules)\n",
    "* [Exercises](#Exercises)\n",
    "  * [Exercise - Maximum Likelihood](#Exercise---Maximum-Likelihood)\n",
    "  * [Plots](#Plots)\n",
    "* [Licenses](#Licenses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Given a sample from a Gaussian distribution, we already know the equations to calculate the best estimate for the expected value $\\hat\\mu$ and the variance $\\hat\\sigma_N^2$ (or $\\sigma_{N-1}^2$). In this notebook you will use the *maximum likelihood estimator (MLE)* to numerically find probabilities for the mean and the variance, given a sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: In order to detect errors in your own code, execute the notebook cells containing `assert` or `assert_almost_equal`. These statements raise exceptions, as long as the calculated result is not yet correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "### Knowledge\n",
    "\n",
    "To complete this exercise notebook, you should possess knowledge about the following topics.\n",
    "- Univariate Gaussian\n",
    "- Maximum Likelihood\n",
    "\n",
    "The following material can help you to acquire this knowledge:\n",
    "\n",
    "* Gaussian, variance, mean:\n",
    " * Chapter 3 of the [Deep Learning Book](http://www.deeplearningbook.org/) [GOO16]\n",
    " * Chapter 1 of the book Pattern Recognition and Machine Learning by Christopher M. Bishop [BIS07]\n",
    "* Univariate gaussian:\n",
    " * [Video1](https://www.khanacademy.org/math/statistics-probability/modeling-distributions-of-data/more-on-normal-distributions/v/introduction-to-the-normal-distribution) and the follwoing of Khan Academy [KHA18a]\n",
    "* Sample variance:\n",
    " * [Video2](https://www.khanacademy.org/math/ap-statistics/summarizing-quantitative-data-ap/measuring-spread-quantitative/v/sample-variance) and the follwoing of Khan Academy [KHA18b]\n",
    "* Read Chapter 24.1 of David MacKays [Book](http://www.inference.org.uk/itprnn/book.html)[MAC03] **(highly recommended,  if you want to dive deeper!)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# External Modules\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Maximum Likelihood\n",
    "\n",
    "*Advice:\n",
    "Read Chapter 24.1 of David MacKays [Book](http://www.inference.org.uk/itprnn/book.html)[MAC03]*\n",
    "\n",
    "The equation to calculate $\\hat \\sigma_{N}^2$ (but also $\\hat\\mu$) can be derived with the *maximum likelihood estimator* for the Gaussian, though this will not be the task here. Instead, we will use it to visualize the most likely values for $\\hat \\sigma_{N}^2$ and $\\hat\\mu$ in a contour plot.\n",
    "\n",
    "#### Recap - Univariate Gaussian\n",
    "\n",
    "The equation for the PDF of a Gaussian is:\n",
    "\n",
    "$$\n",
    "P(x\\mid\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "with:\n",
    "- the mean $\\mu$\n",
    "- the standard deviation $\\sigma$\n",
    "\n",
    "#### Recap - Maximum Likelihood\n",
    "\n",
    "The *maximum likelihood estimator* is a method which finds a point estimate for the parameters $\\theta$ for a known function $p({\\bf x}\\mid\\theta)$ given an observation $\\bf x$:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^N p(x_i\\mid\\theta)\n",
    "$$\n",
    "\n",
    "The parameters $\\theta$, which maximize this function are most likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (pen & paper):**\n",
    "\n",
    "1. Write down the likelihood function for the normal distribution.\n",
    "2. Show that the likelihood for the normal distribution can be written as:\n",
    "\n",
    "$$\n",
    "L(\\mu, \\sigma^2) = \\prod_{i=1}^N p(x_i\\mid\\mu,\\sigma^2) = \\ldots =\n",
    "\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^N \\exp\\left(-\\frac{N(\\mu- \\hat\\mu)^2 + S}{2\\sigma^2}\\right)\n",
    "$$\n",
    "with:\n",
    " - the empirical mean $ \\hat\\mu$ of the observation $\\hat\\mu = \\frac{1}{N}\\sum x_i$\n",
    " - $S = \\sum_{i=1}^{N}(x_i - \\hat\\mu)^2$\n",
    "\n",
    "The purpose of this rewritten equation is, that it can be implemented way more efficient than the original equation from Task 1.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "For Task 2: Make use of the equation to calculate $\\hat \\sigma_{N}^2 =  \\frac{1}{N} \\sum_{x_i}^N \\left( x_i - \\hat{\\mu} \\right)^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "\n",
    "Implement the function from the last exercise (2.) to calculate the likelihood for concrete values for $\\mu$ and $\\sigma$ given $\\bf x$. Use the predefined 2D arrays generated with `np.meshgrid` in order to avoid loops in your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = -1.5\n",
    "sigma = 3\n",
    "sigma_square = sigma**2\n",
    "size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(mu, sigma_square, size):\n",
    "    sigma = np.sqrt(sigma_square)\n",
    "    x = np.random.normal(loc=mu, scale=sigma, size=size)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_data(mu, sigma_square, size)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gaussian_pdf(mu, sigma):\n",
    "    x_ = np.linspace(mu - 4*sigma, mu + 4*sigma, 100)\n",
    "    plt.plot(x_,scipy.stats.norm.pdf(x_, mu, sigma))\n",
    "    plt.plot(x,np.zeros_like(x), \"ro\")\n",
    "plot_gaussian_pdf(mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ = x.mean()\n",
    "sigma_ = np.sqrt(np.var(x,ddof=1))\n",
    "xlist = np.linspace(mean_-1, mean_+1, 100)\n",
    "ylist = np.linspace(sigma_-1, sigma_+1., 100)\n",
    "X, Y = np.meshgrid(xlist, ylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def likelihood_gaussian(x, mu, sigma):\n",
    "    \"\"\" Calculates the likelihood for univariate Gaussian\n",
    "    \n",
    "    :x: sample as 1D numpy-array (float)\n",
    "    :mu: values for the mean as 2D numpy-array (float) with the shape (n,n)\n",
    "        [[m1,m2,...,mn], [m1,m2,...,mn], ..., [m1,m2,...,mn]]\n",
    "    :sigma: values for sigma as 2D numpy-array (float) with the shape (n,n)\n",
    "        [[s1,s1,...,s1], [s2,s2,...,s2], ..., [sn,sn,...,sn]]\n",
    "    \n",
    "    :returns: probabilities as 2D numpy-array (float)\n",
    "    \"\"\"  \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = likelihood_gaussian(x, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots\n",
    "\n",
    "With the use of the function we can now plot:\n",
    "- The likelihood for the Gaussian for different $\\mu$ and $\\sigma$, given $\\bf x$.\n",
    "- The posterior probability of $\\mu$ for different fixed values  $\\sigma$.\n",
    "- The posterior probability of $\\sigma$ for different fixed values  $\\mu$.\n",
    "\n",
    "If your implementation `likelihood_gaussian` is correct, the plots should look similiar to these:\n",
    "\n",
    "<img src=\"https://gitlab.com/deep.TEACHING/educational-materials/raw/dev/media/klaus/exercise-univariate-gaussian-contour-sigma-mu.png\" width=\"800\" alt=\"internet connection needed\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "cp = plt.contour(X, Y, Z)\n",
    "\n",
    "plt.title('Likelihood')\n",
    "plt.xlabel('$\\mu$')\n",
    "plt.ylabel('$\\sigma$')\n",
    "plt.show()\n",
    "\n",
    "print(\"For comparison:\")\n",
    "print(\"calculated mean:\\t\", x.mean())\n",
    "print(\"calculated sigma:\\t\", np.sqrt(x.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = 1000\n",
    "mu = np.linspace(-5,5,n_values)\n",
    "sigma_2 = likelihood_gaussian(x, mu, sigma=2)\n",
    "sigma_2 = sigma_2 / sigma_2.sum()\n",
    "sigma_2_5 = likelihood_gaussian(x, mu, sigma=2.5)\n",
    "sigma_2_5 = sigma_2_5 / sigma_2_5.sum()\n",
    "sigma_3 = likelihood_gaussian(x, mu, sigma=3.)\n",
    "sigma_3 = sigma_3 / sigma_3.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_factor = n_values/10\n",
    "plt.plot(mu, sigma_2*scaling_factor, 'b-', label=\"$\\sigma=2$\")\n",
    "plt.plot(mu, sigma_2_5*scaling_factor, 'g-', label=\"$\\sigma=2.5$\")\n",
    "plt.plot(mu, sigma_3*scaling_factor, 'r-', label=\"$\\sigma=3$\")\n",
    "plt.xlabel('$\\mu$')\n",
    "plt.ylabel('$p(\\mu\\mid x,\\sigma)$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.linspace(0.1,7,n_values)\n",
    "mu_m0_15 = likelihood_gaussian(x, mu=-1.0, sigma=sigma)\n",
    "mu_m0_15 = mu_m0_15 / mu_m0_15.sum()\n",
    "mu_0_4 = likelihood_gaussian(x, mu=0.0, sigma=sigma)\n",
    "mu_0_4 = mu_0_4 / mu_0_4.sum()\n",
    "mu_0_8 = likelihood_gaussian(x, mu=1.0, sigma=sigma)\n",
    "mu_0_8 = mu_0_8 / mu_0_8.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sigma, mu_m0_15*scaling_factor, 'b-', label=\"$\\mu=-1.0$\")\n",
    "plt.plot(sigma, mu_0_4*scaling_factor, 'g-', label=\"$\\mu=0.0$\")\n",
    "plt.plot(sigma, mu_0_8*scaling_factor, 'r-', label=\"$\\mu=1.$\")\n",
    "plt.xlabel('$\\sigma$')\n",
    "plt.ylabel('$p(\\sigma\\mid x,\\mu)$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Literature\n",
    "\n",
    "<table>\n",
    "        <tr>\n",
    "        <td>\n",
    "            <a name=\"BIS07\"></a>[BIS07]\n",
    "        </td>\n",
    "        <td>\n",
    "            Christopher M. Bishop, Pattern recognition and machine learning, 5th Edition. Springer 2007, ISBN 9780387310732.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a name=\"GOO16\"></a>[GOO16]\n",
    "        </td>\n",
    "        <td>\n",
    "            Goodfellow, Ian, et al. Deep learning. Vol. 1. Cambridge: MIT press, 2016.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a name=\"MAC03\"></a>[MAC03]\n",
    "        </td>\n",
    "        <td>\n",
    "            MacKay, David JC, and David JC Mac Kay. Information theory, inference and learning algorithms. Cambridge university press, 2003.\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            <a name=\"KHA18a\"></a>[KHA18a]\n",
    "        </td>\n",
    "        <td>\n",
    "            Khan Academy. (2018, October 05). Deep definition of the normal distribution. And following in the playlist. Retrieved from https://www.khanacademy.org/math/statistics-probability/modeling-distributions-of-data/more-on-normal-distributions/v/introduction-to-the-normal-distribution\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            <a name=\"KHA18b\"></a>[KHA18b]\n",
    "        </td>\n",
    "        <td>\n",
    "            Khan Academy. (2018, October 05). Deep definition of the normal distribution. And following in the playlist. Retrieved from https://www.khanacademy.org/math/ap-statistics/summarizing-quantitative-data-ap/measuring-spread-quantitative/v/sample-variance\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Licenses\n",
    "\n",
    "### Notebook License (CC-BY-SA 4.0)\n",
    "\n",
    "*The following license applies to the complete notebook, including code cells. It does however not apply to any referenced external media (e.g., images).*\n",
    "\n",
    "Exercise - Multivariate Gaussian <br/>\n",
    "by Christian Herta, Klaus Strohmenger<br/>\n",
    "is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).<br/>\n",
    "Based on a work at https://gitlab.com/deep.TEACHING.\n",
    "\n",
    "\n",
    "### Code License (MIT)\n",
    "\n",
    "*The following license only applies to code cells of the notebook.*\n",
    "\n",
    "Copyright 2018 Christian Herta, Klaus Strohmenger\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "deep_teaching_kernel",
   "language": "python",
   "name": "deep_teaching_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
